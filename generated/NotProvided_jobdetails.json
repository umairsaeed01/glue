{
  "job_title": "Data Engineer",
  "job_purpose": "To scale and embed the Unified Data Engineering (UDE) Code Framework across the enterprise, enabling delivery teams to build more efficiently and consistently.",
  "keywords": [
    "Data engineering",
    "Python",
    "Design patterns",
    "Modular software development",
    "Airflow",
    "Kafka/Confluent",
    "dbt",
    "Snowflake",
    "Great Expectations",
    "Git/GitLab",
    "CI/CD",
    "Terraform/IaC",
    "Data contracts",
    "Metadata-driven pipelines",
    "Declarative pipeline configuration"
  ],
  "job_duties_and_responsibilities": [
    "Operationalize reusable, AI-ready, and governed pipelines",
    "Evolve the framework and embed software engineering best practices in code, tooling, and automation",
    "Work closely with domain-aligned experts",
    "Scale and embed the UDE Code Framework",
    "Enable delivery teams to build more efficiently and consistently"
  ],
  "required_qualifications": [
    "Experience in data engineering with production-grade pipeline delivery",
    "Proficiency in Python and familiarity with design patterns and modular software development practices",
    "Experience with Airflow (Astronomer preferred), Kafka/Confluent, dbt, Snowflake, Great Expectations",
    "Strong Git/GitLab skills and experience with CI/CD and Terraform/IaC concepts",
    "Exposure to data contracts, metadata-driven pipelines, or declarative pipeline configuration"
  ],
  "preferred_qualifications": [],
  "company_name": "Not provided",
  "company_details": "This is a Perth based role. No sponsorship is available."
}